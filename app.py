import gradio as gr
import whisper
from groq import Groq
from gtts import gTTS
import os
from dotenv import load_dotenv

load_dotenv()

# Load Whisper model
whisper_model = whisper.load_model("small")

# Initialize Groq client
client = Groq(api_key=os.getenv("GROQ_API_KEY"))

# Emotion to color and emoji mapping
emotion_ui = {
    "happy": ("#FFF9C4", "ğŸ˜ƒ"),
    "sad": ("#BBDEFB", "ğŸ˜”"),
    "angry": ("#FFCDD2", "ğŸ˜¡"),
    "confused": ("#E1BEE7", "ğŸ¤”"),
    "fearful": ("#D1C4E9", "ğŸ˜±"),
    "surprised": ("#C8E6C9", "ğŸ˜®"),
    "neutral": ("#F5F5F5", "ğŸ˜"),
}


def get_emotion_from_text(text):
    response = client.chat.completions.create(
        model="llama3-70b-8192",
        messages=[
            {"role": "system", "content": "You are an emotion detection assistant. Respond with only one emotion like: happy, sad, angry, fearful, surprised, confused, or neutral."},
            {"role": "user", "content": text}
        ]
    )
    return response.choices[0].message.content.strip().lower()


def voice_assistant(audio):
    # Step 1: Transcribe
    result = whisper_model.transcribe(audio)
    transcription = result["text"]

    # Step 2: Detect emotion
    emotion = get_emotion_from_text(transcription)
    color, emoji = emotion_ui.get(emotion, ("#FFFFFF", "ğŸ¤·"))

    # Step 3: Response based on emotion
    responses = {
        "angry": "I'm really sorry you're facing trouble. Letâ€™s sort this out right away.",
        "happy": "I'm so glad to hear that! Let me know how I can help further.",
        "sad": "Iâ€™m here for you. Letâ€™s see how I can assist.",
        "confused": "No worries, Iâ€™ll try to explain things more clearly.",
        "fearful": "Itâ€™s okay to be scared. Iâ€™m here to help.",
        "surprised": "Wow, thatâ€™s unexpected! Letâ€™s explore more.",
        "neutral": "Got it! How can I assist you today?",
    }
    response = responses.get(emotion, "Got it! How can I assist you today?")

    # Step 4: Generate voice response
    tts = gTTS(text=response)
    tts_path = "response.mp3"
    tts.save(tts_path)

    # Return values
    return (
        transcription,
        f"{emoji} {emotion.capitalize()}",
        response,
        color,
        tts_path
    )


# Gradio Blocks with dynamic background
with gr.Blocks() as demo:
    gr.Markdown("## ğŸ¤– Emotion-Adaptive Voice Assistant")
    audio_input = gr.Audio(type="filepath", label="ğŸ¤ Speak Now")
    with gr.Row():
        transcription_output = gr.Textbox(label="ğŸ“ Transcription")
        emotion_output = gr.Textbox(label="ğŸ§  Emotion")
    response_output = gr.Textbox(label="ğŸ’¡ Assistant Reply")
    voice_output = gr.Audio(label="ğŸ§ Assistant Voice", interactive=False)
    color_box = gr.HTML()

    def wrapped(audio):
        transcription, emotion, response, color, voice = voice_assistant(audio)
        box_style = f'<div style="background-color:{color}; padding:20px; border-radius:10px">Emotion color applied: {emotion}</div>'
        return transcription, emotion, response, voice, box_style

    audio_input.change(fn=wrapped, inputs=audio_input,
                       outputs=[transcription_output, emotion_output, response_output, voice_output, color_box])

if __name__ == "__main__":
    demo.launch()
